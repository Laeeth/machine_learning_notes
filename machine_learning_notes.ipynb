{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS/ User specific \n",
    "import os\n",
    "# DATASET_PATH = \"/home/jcharlet/workspace/datascience/machine_learning_notes/data/\"\n",
    "DATASET_PATH = \"/opt/dataiku-dss/config/ipython_notebooks/JCKAGGLEHOUSEPRICES/data/\" \n",
    "# DATASET_PATH = os.path.abspath(os.path.join( os.getcwd() , 'data')) + r'\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle project - Table of Contents\n",
    "* 1 - Define the problem\n",
    "* [Load data and displaying info](#load-data)\n",
    "* 2 - Prepare Data\n",
    "    * [Identify features](#identify)\n",
    "        * Separate numerical from categorical features\n",
    "        * Separate nominal and ordinal (from categorical features)\n",
    "    * [Clean data](#clean)\n",
    "        * Remove numerical features with missing values\n",
    "        * Remove categorical features with missing values\n",
    "        * drop outliers in numerical values # WIP\n",
    "    * [transform](#transform) # TODO\n",
    "        * transform categorical values #TODO\n",
    "* 2 - [Feature selection](#feature-selection) #WIP\n",
    "    * Select features using random forest classifier #WIP\n",
    "    \n",
    "* 3 - [Spot Check Algorithms](#spot-check)\n",
    "    * split dataset\n",
    "    * train on multiple algorithms  # WIP\n",
    "    \n",
    "    \n",
    "## 7/8: next step is to work on categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From project https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "\n",
    "- Step 1: What is the problem? \n",
    "\n",
    "house prices are hard to predict, we want to know how much my beautiful flat is worth on the market in the USA\n",
    "\n",
    "- Step 2: Why does the problem need to be solved? \n",
    "\n",
    "MONEY MONEY\n",
    "and especially we need to learn Machine Learning\n",
    "\n",
    "- Step 3: How would I solve the problem? \n",
    "\n",
    "housing agents do statistical studies.\n",
    "\n",
    "Data pre-processing is very important. We have a lot of features, dirty data (missing values), numerical/ordinal/nominal values.\n",
    "\n",
    "We'll follow the process from https://machinelearningmastery.com/process-for-working-through-machine-learning-problems/\n",
    "\n",
    "- 1 - Define the Problem\n",
    "- 2 - Prepare Data\n",
    "- 3 - Spot Check Algorithms\n",
    "- 4 - Improve Results\n",
    "- 5 - Present Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data <a class=\"anchor\" id=\"load-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### PLOTS\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "\n",
    "### Split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#training\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "\n",
    "# standardization\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# training evaluation\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET_PATH + \"train.csv\")\n",
    "df_test = pd.read_csv(DATASET_PATH + \"test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data <a class=\"anchor\" id=\"clean-data\"></a>\n",
    "## Identify features<a class=\"anchor\" id=\"identify\"></a>\n",
    "### Separate numerical from categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating data features according to data type: numeric or string\n",
    "\n",
    "X_numeric_labels = set(df._get_numeric_data().columns.tolist())\n",
    "X_categorical_labels = set(df.columns.tolist()).difference(X_numeric_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('These are numerical features {} \\n'.format(X_numeric_labels))\n",
    "print('These are categorical features {} \\n'.format(X_categorical_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate nominal and ordinal (from categorical features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating categorical features in two sets: nominal and ordinal\n",
    "# The values in features asociated with categorical ordinal feautures are: [Ex, Gd, GLQ, GdPrv, Fin] ... etc\n",
    "\n",
    "categorical_ordinal_list_values = ['Ex', 'Gd', 'GLQ', 'GdPrv', 'Fin'] # There are not all of the values. Here must be\n",
    "# included all the possible values for the categorical ordinal features\n",
    "\n",
    "X_categorical_nominal_labels = (df[list(X_categorical_labels)].isin(categorical_ordinal_list_values) == False).all().loc[lambda df: df.values == True].axes\n",
    "\n",
    "# The previous instruction returns the index of the categorical_nominal_values. I think we could include the following features:\n",
    "#LandSlope has values: Gtl -> Gentle slopen; Mod -> Moderate Slope; Sev -> Severe Slope\n",
    "#Functional has values: Typ -> Typical Functionality; Min1 -> Minor Deductions 1; Min2 -> Minor Deductions 2; Mod -> Moderate Deductions; Maj1 -> Major Deductions 1; Maj2 -> Major Deductions 2; Sev -> Severely Damaged; Sal -> Salvage only\n",
    "#PavedDrive has values:   Y -> Paved ; P -> Partial Pavement; N -> Dirt/Gravel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(categorical_ordinal_list_values)\n",
    "print()\n",
    "print(X_categorical_nominal_labels[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "<p> From the set of set of **X_categorical_labels** the set of **X_categorical_nominal_labels** is extracted to get \n",
    "the set of **X_categorical_ordinal_labels**</p>\n",
    "\n",
    "* On the set of **X_categorical_ordinal_labels** we can apply pag 104 **mapping strategy**\n",
    "\n",
    "* On the set of **X_categorical_nominal_labels** we can apply **one-hot encoding strategy** pag 106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_categorical_ordinal_labels = set(X_categorical_labels).difference(set(X_categorical_nominal_labels[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_categorical_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[list(X_categorical_ordinal_labels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data<a class=\"anchor\" id=\"clean\"></a>\n",
    "\n",
    "#### Some considerations\n",
    "\n",
    "\n",
    "* <p> Page 107 of \"our book\":  \"After executing the preceding code, the first column of the NumPy array X now holds the new\n",
    "colour values, which are encoded as follows:\n",
    "blue to 0\n",
    "green to 1\n",
    "red to 2\n",
    "If we stop at this point and feed the array to our classifier, *we will make one of the most common \n",
    "mistakes in dealing with categorical data*. Can you spot the problem? *\n",
    "Although the colour values do not come in any particular order, a learning algorithm will now assume\n",
    "that green is larger than blue, and red is larger than green. \n",
    "Although this assumption is incorrect, the algorithm could still produce useful results. \n",
    "However, those results would not be optimal* \"</p>\n",
    "\n",
    "* <p> In the numerical feature variables there is the possibility that there are categorical numerical features variables. How to identify this kinf of features ? \n",
    " I think the main problem is what strategy to use to replace values ? median, mode or mean ? </p>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned=df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove categorical features with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are dropping any categorical feature with at least one null value, we can relax this requirement by changing df > 0 to \n",
    "# df > n with n > 0\n",
    "\n",
    "X_categorical_missing_values_labels = \\\n",
    "        df[list(X_categorical_labels)].isnull().sum().loc[lambda df: df > 0].index.tolist()\n",
    "X_categorical_not_missing_values_labels=[x for x in X_categorical_labels if x not in X_categorical_missing_values_labels]\n",
    "\n",
    "print(X_categorical_missing_values_labels)\n",
    "\n",
    "print(\" Number of Total Categorical Values = {} \\n Number of Categorical Missing Values = {}  \\n Difference = {}\"\\\n",
    "      .format(len(X_categorical_labels), len(X_categorical_missing_values_labels), \\\n",
    "              len(X_categorical_labels)- len(X_categorical_missing_values_labels) ))\n",
    "\n",
    "df_cleaned.drop(X_categorical_missing_values_labels, 1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove numerical features with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numerical_missing_values_labels=df_cleaned[df_cleaned._get_numeric_data().columns.tolist()].isnull().sum()\\\n",
    "                    .loc[lambda df: df.values > 0].index.tolist()\n",
    "X_numerical_not_missing_values_labels=[x for x in X_numeric_labels if x not in X_numerical_missing_values_labels]\n",
    "\n",
    "print(\" Number of Total Numerical Values = {} \\n Number of Numerical Missing Values = {}  \\n Difference = {}\"\\\n",
    "      .format(len(X_numeric_labels), len(X_numerical_missing_values_labels), \\\n",
    "              len(X_numeric_labels)- len(X_numerical_missing_values_labels) ))\n",
    "\n",
    "\n",
    "df_cleaned.drop(X_numerical_missing_values_labels, 1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_numerical_missing_values_labels)\n",
    "# print(categorical_missing_values_labels)\n",
    "# df_cleaned.columns.values\n",
    "# df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop outliers in numerical values # WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def drop_outliers(df, field_name):\n",
    "#     distance = 1.5 * (np.percentile(df[field_name], 75) - np.percentile(df[field_name], 25))\n",
    "#     df.drop(df[df[field_name] > distance + np.percentile(df[field_name], 75)].index, inplace=True)\n",
    "#     df.drop(df[df[field_name] < np.percentile(df[field_name], 25) - distance].index, inplace=True)\n",
    "\n",
    "# # drop_outliers(df_cleaned, 'LotArea')    \n",
    "# df_cleaned_no_outlier = df.copy()\n",
    "# for feature in X_numerical_not_missing_values_labels:\n",
    "#     drop_outliers(df_cleaned_no_outlier, feature)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"nb of rows with outliers:{}, without: {}\".format(df.shape[0],df_cleaned_no_outlier.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are currently removing far too many items.\n",
    "\n",
    "attempt above was removing values using univariate method: unsatisfying: we remove a third of the dataset, we should rather use filters here if we really want to deal with outliers on 1 dimension\n",
    "\n",
    "but a much better way would be to find outliers in the whole dataset on all dimensions using multivariate method. We would then remove those outliers from the training set to improve accuracy\n",
    "\n",
    "see http://scikit-learn.org/stable/modules/outlier_detection.html\n",
    "http://scikit-learn.org/stable/auto_examples/covariance/plot_outlier_detection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform<a class=\"anchor\" id=\"transform\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.set_index(\"Id\")\n",
    "dfTransformed=pd.get_dummies(df_cleaned.copy())\n",
    "dfTransformed.drop(\"Id\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTransformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Feature Selection<a class=\"anchor\" id=\"feature-selection\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_columns=filter(lambda x: x !=\"SalePrice\" and x!=\"Id\", df.columns)\n",
    "# feature_numerical_columns = df[feature_columns].select_dtypes(include=[np.number]).columns.tolist()\n",
    "feature_columns=[x for x in dfTransformed if x !=\"SalePrice\"]\n",
    "# feature_columns=X_numerical_not_missing_values_labels\n",
    "# feature_columns.extend(X_categorical_not_missing_values_labels)\n",
    "prediction_column=\"SalePrice\"\n",
    "# # print(feature_columns)\n",
    "X = dfTransformed[feature_columns].values\n",
    "Y=dfTransformed[[prediction_column]].values;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier();\n",
    "model = model.fit(X, Y)    \n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"label {} : {}\".format(feature_columns[f],importances[indices[f]]))\n",
    "# print(indices)\n",
    "\n",
    "# plt.bar(range(X.shape[1]),importances[indices],align='center')\n",
    "# plt.xticks(range(X.shape[1]),feature_numerical_columns,rotation=90);\n",
    "# plt.rcParams[\"figure.figsize\"] = (17,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To draw graph and select features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spot Check Algorithms<a class=\"anchor\" id=\"spot-check\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataSet(df,feature_columns):\n",
    "    X = df[feature_columns].values\n",
    "    Y=df[\"SalePrice\"].values;\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.33,random_state=42)\n",
    "    # print(Y_train.head())\n",
    "    # X_train.head()\n",
    "    return X_train,X_test,Y_train,Y_test\n",
    "\n",
    "\n",
    "def trainOnMultipleModels(X_train,X_test,Y_train,Y_test,models,modelsWhichNeedNormalization):\n",
    "    \n",
    "    print(\"5 expected values: {}\\n\".format(Y_test[0:5].tolist()))\n",
    "    containers=[]\n",
    "    for name,model in models:\n",
    "    #     Train the model using the training sets\n",
    "        model.fit(X_train, Y_train)\n",
    "\n",
    "        # Make predictions using the testing set\n",
    "        y_pre = model.predict(X_test)\n",
    "        mean_squared_err= mean_squared_error(Y_test, y_pre)\n",
    "        print(\"Training on {}\\n   mean squared log error: {}\\n\".format(name,mean_squared_err))\n",
    "        print(\"   5 predicted values: {}\\n\".format(y_pre[0:5]))\n",
    "        containers.append([name,model,y_pre,mean_squared_err])\n",
    "\n",
    "#     print(\"5 expected values: {}\\n\".format(Y_test[0:5].tolist()))\n",
    "    for name,model in modelsWhichNeedNormalization:\n",
    "    #     Train the model using the training sets\n",
    "        sc=StandardScaler()\n",
    "        sc.fit(X_train)\n",
    "\n",
    "        X_train_scaled = sc.transform(X_train)\n",
    "        model.fit(X_train_scaled, Y_train)\n",
    "\n",
    "        # Make predictions using the testing set\n",
    "        X_test_scaled = sc.transform(X_test)\n",
    "        y_pre = model.predict(X_test_scaled)\n",
    "\n",
    "        mean_squared_err= mean_squared_error(Y_test, y_pre)\n",
    "        print(\"Training on {}\\n   mean squared log error: {}\\n\".format(name,mean_squared_err))\n",
    "        print(\"   5 predicted values: {}\\n\".format(y_pre[0:5]))\n",
    "        containers.append([name,model,y_pre,mean_squared_err])\n",
    "    return containers\n",
    "\n",
    "def displayGraphTrueVsPred(name,y_pre,Y_test):\n",
    "    plt.rcParams[\"figure.figsize\"] = (7,7)\n",
    "\n",
    "    preds_df=pd.DataFrame(data=y_pre,columns=[\"predictions\"])\n",
    "    preds_df[\"true values\"]=Y_test\n",
    "\n",
    "    print(\"Model: {}\".format(name))\n",
    "\n",
    "    plt.plot(preds_df[\"predictions\"], preds_df[\"true values\"],'bo')\n",
    "    plt.plot([0,700000], [0,700000])\n",
    "    # line1, = plt.plot(y_pre, Y_test, 'b', label='predictions')\n",
    "    # plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "    # plt.ylabel('Sales price')\n",
    "    plt.xlabel('Predictions')\n",
    "    plt.ylabel('true values')\n",
    "    # plt.yscale(\"log\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def displayGraphTrueVsPred2(name,y_pre,Y_test):\n",
    "    plt.rcParams[\"figure.figsize\"] = (17,4)\n",
    "\n",
    "    print(\"Model: {}\".format(name))\n",
    "\n",
    "    preds_df=pd.DataFrame(data=y_pre,columns=[\"predictions\"])\n",
    "    preds_df[\"true values\"]=Y_test\n",
    "    preds_df.sort_values(by=\"true values\",inplace=True)\n",
    "    preds_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "    line1, = plt.plot(preds_df[\"predictions\"], 'b',linewidth=2)\n",
    "    line2, = plt.plot(preds_df[\"true values\"], 'c')\n",
    "    plt.legend(handler_map={line1: HandlerLine2D(numpoints=1)})\n",
    "    plt.ylabel('Sales price')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def displayErrGraphOnAllAlgorithms(containers):\n",
    "    plt.rcParams[\"figure.figsize\"] = (14,2)\n",
    "\n",
    "    names=[(lambda row: row[0])(row) for row in containers]\n",
    "    mean_squared_errs=[(lambda row: row[3])(row) for row in containers]\n",
    "\n",
    "    errs_df=pd.DataFrame(data=mean_squared_errs,columns=[\"err\"])\n",
    "    errs_df[\"name\"]=names\n",
    "    errs_df.sort_values(by=\"err\",inplace=True)\n",
    "\n",
    "    line1, = plt.plot(errs_df[\"name\"],errs_df[\"err\"], 'b', label='Train f1 score on lying straight')\n",
    "    plt.legend(handler_map={line1: HandlerLine2D(numpoints=1)})\n",
    "    plt.ylabel('Mean squared error')\n",
    "    plt.xlabel('Model')\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try on numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append((\"SVC\",SVC()))\n",
    "models.append((\"LinearSVC\",LinearSVC()))\n",
    "models.append((\"KNeighbors\",KNeighborsClassifier()))\n",
    "models.append((\"DecisionTree\",DecisionTreeClassifier()))\n",
    "models.append((\"RandomForest\",RandomForestClassifier()))\n",
    "\n",
    "modelsWhichNeedNormalization = []\n",
    "modelsWhichNeedNormalization.append((\"LogisticRegression\",LogisticRegression()))\n",
    "modelsWhichNeedNormalization.append((\"MLPClassifier\",MLPClassifier(solver='lbfgs', random_state=0)))\n",
    "modelsWhichNeedNormalization.append((\"LinearRegression\", LinearRegression()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Id\" in X_numerical_not_missing_values_labels:\n",
    "    X_numerical_not_missing_values_labels.remove(\"Id\")\n",
    "X_train,X_test,Y_train,Y_test = splitDataSet(dfTransformed,X_numerical_not_missing_values_labels)\n",
    "containers_on_num_features = trainOnMultipleModels(X_train,X_test,Y_train,Y_test,models,modelsWhichNeedNormalization)\n",
    "displayErrGraphOnAllAlgorithms(containers_on_num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_test[0:5])\n",
    "\n",
    "print(containers_on_num_features[7][2][0:5].astype(int))\n",
    "print(containers_on_num_features[2][2][0:5])\n",
    "print(containers_on_num_features[4][2][0:5])\n",
    "\n",
    "# displayGraphTrueVsPred(containers[7][0],containers[7][2],Y_test)\n",
    "# displayGraphTrueVsPred(containers[2][0],containers[2][2],Y_test)\n",
    "displayGraphTrueVsPred2(containers_on_num_features[7][0],containers_on_num_features[7][2],Y_test)\n",
    "displayGraphTrueVsPred2(containers_on_num_features[2][0],containers_on_num_features[2][2],Y_test)\n",
    "displayGraphTrueVsPred2(containers_on_num_features[4][0],containers_on_num_features[4][2],Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try on numerical and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = splitDataSet(dfTransformed,feature_columns)\n",
    "containers_with_num_cat_features = trainOnMultipleModels(X_train,X_test,Y_train,Y_test,models,modelsWhichNeedNormalization)\n",
    "displayErrGraphOnAllAlgorithms(containers_with_num_cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# displayGraphTrueVsPred(containers[7][0],containers[7][2],Y_test)\n",
    "# displayGraphTrueVsPred(containers[2][0],containers[2][2],Y_test)\n",
    "displayGraphTrueVsPred2(containers_with_num_cat_features[7][0],containers_with_num_cat_features[7][2],Y_test)\n",
    "displayGraphTrueVsPred2(containers_with_num_cat_features[2][0],containers_with_num_cat_features[2][2],Y_test)\n",
    "displayGraphTrueVsPred2(containers_with_num_cat_features[4][0],containers_with_num_cat_features[4][2],Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "displayErrGraphOnAllAlgorithms(containers_on_num_features)\n",
    "displayErrGraphOnAllAlgorithms(containers_with_num_cat_features)\n",
    "displayGraphTrueVsPred2(containers_on_num_features[7][0],containers_on_num_features[7][2],Y_test)\n",
    "displayGraphTrueVsPred2(containers_on_num_features[2][0],containers_on_num_features[2][2],Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env python_3_5)",
   "language": "python",
   "name": "py-dku-venv-python_3_5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
